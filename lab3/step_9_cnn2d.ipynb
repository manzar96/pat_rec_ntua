{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "805b01c047177ae5e450b0f33ddfc5da772417ef"
   },
   "source": [
    "**Βήμα 9:** CNN 2d με τις προδιαγραφές που ζητήθηκαν.\n",
    "Ζητείται να εκπαιδευτεί και στο validation και να αξιολογηθεί μόνο στο test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "import os\n",
    "\n",
    "class_mapping = {\n",
    "    'Rock': 'Rock',\n",
    "    'Psych-Rock': 'Rock',\n",
    "    'Indie-Rock': None,\n",
    "    'Post-Rock': 'Rock',\n",
    "    'Psych-Folk': 'Folk',\n",
    "    'Folk': 'Folk',\n",
    "    'Metal': 'Metal',\n",
    "    'Punk': 'Metal',\n",
    "    'Post-Punk': None,\n",
    "    'Trip-Hop': 'Trip-Hop',\n",
    "    'Pop': 'Pop',\n",
    "    'Electronic': 'Electronic',\n",
    "    'Hip-Hop': 'Hip-Hop',\n",
    "    'Classical': 'Classical',\n",
    "    'Blues': 'Blues',\n",
    "    'Chiptune': 'Electronic',\n",
    "    'Jazz': 'Jazz',\n",
    "    'Soundtrack': None,\n",
    "    'International': None,\n",
    "    'Old-Time': None\n",
    "}\n",
    "\n",
    "\n",
    "def torch_train_val_split(\n",
    "        dataset, batch_train, batch_eval,\n",
    "        val_size=.2, shuffle=True, seed=42):\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    val_split = int(np.floor(val_size * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices = indices[val_split:]\n",
    "    val_indices = indices[:val_split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset,\n",
    "                              batch_size=batch_train,\n",
    "                              sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_eval,\n",
    "                            sampler=val_sampler)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def read_spectrogram(spectrogram_file, chroma=True):\n",
    "    with gzip.GzipFile(spectrogram_file, 'r') as f:\n",
    "        spectrograms = np.load(f)\n",
    "    # spectrograms contains a fused mel spectrogram and chromagram\n",
    "    # Decompose as follows\n",
    "    return spectrograms.T\n",
    "\n",
    "\n",
    "class LabelTransformer(LabelEncoder):\n",
    "    def inverse(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).inverse_transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).inverse_transform([y])\n",
    "\n",
    "    def transform(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).transform([y])\n",
    "\n",
    "        \n",
    "class PaddingTransform(object):\n",
    "    def __init__(self, max_length, padding_value=0):\n",
    "        self.max_length = max_length\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def __call__(self, s):\n",
    "        if len(s) == self.max_length:\n",
    "            return s\n",
    "\n",
    "        if len(s) > self.max_length:\n",
    "            return s[:self.max_length]\n",
    "\n",
    "        if len(s) < self.max_length:\n",
    "            s1 = copy.deepcopy(s)\n",
    "            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n",
    "            s1 = np.vstack((s1, pad))\n",
    "            return s1\n",
    "\n",
    "        \n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, path, class_mapping=None, train=True, max_length=-1):\n",
    "        t = 'train' if train else 'test'\n",
    "        p = os.path.join(path, t)\n",
    "        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n",
    "        #print(self.index)\n",
    "        self.files, labels = self.get_files_labels(self.index, class_mapping)\n",
    "        self.feats = [read_spectrogram(os.path.join(p, f)) for f in self.files]\n",
    "        self.feat_dim = self.feats[0].shape[1]\n",
    "        self.lengths = [len(i) for i in self.feats]\n",
    "        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n",
    "        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n",
    "        self.label_transformer = LabelTransformer()\n",
    "        if isinstance(labels, (list, tuple)):\n",
    "            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n",
    "\n",
    "    def get_files_labels(self, txt, class_mapping):\n",
    "        with open(txt, 'r') as fd:\n",
    "            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n",
    "        files, labels = [], []\n",
    "        for l in lines:\n",
    "            label = l[1]\n",
    "            if class_mapping:\n",
    "                label = class_mapping[l[1]]\n",
    "            if not label:\n",
    "                continue\n",
    "            files.append(l[0])\n",
    "            labels.append(label)\n",
    "        return files, labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        l = min(self.lengths[item], self.max_length)\n",
    "        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3edc020fa2a26961b7df43a0acd6ff12414fe399"
   },
   "outputs": [],
   "source": [
    "BATCH_SZ=32\n",
    "\n",
    "specs = SpectrogramDataset('../input/data/data/fma_genre_spectrograms/', train=True, class_mapping=class_mapping, max_length=-1)\n",
    "train_loader, val_loader = torch_train_val_split(specs, BATCH_SZ ,BATCH_SZ, val_size=0)\n",
    "test_loader = DataLoader(SpectrogramDataset('../input/data/data/fma_genre_spectrograms/', train=False, class_mapping=class_mapping, max_length=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "1a2c9edcc155c7592744223554655e621b631011"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,input_channels,out_channels,kernel_sz,stride,padding, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 4, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16 , 32 , kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        \n",
    "        self.dense1= nn.Linear(6720,500) \n",
    "        self.dense2 = nn.Linear(500,10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x,lengths):\n",
    "        #print(x.shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        #print(x.shape)\n",
    "        x.unsqueeze_(1)\n",
    "        #print(x.shape)\n",
    "        out1 = self.layer1(x)\n",
    "        #print(out1.shape)\n",
    "        out2= self.layer2(out1)\n",
    "        #print(out2.shape)\n",
    "        out3= self.layer3(out2)\n",
    "        #print(out3.shape)\n",
    "        out4= self.layer4(out3)\n",
    "        #print(out4.shape)\n",
    "        \n",
    "    \n",
    "        out_flat=out4.reshape(-1,out4.size(1)*out4.size(2)*out4.size(3))\n",
    "        #print(out_flat.shape)\n",
    "        \n",
    "        \n",
    "        #implementing fully connected layers\n",
    "        \n",
    "        hidden_out = self.dense1(out_flat)\n",
    "        final_out = self.dense2(hidden_out)\n",
    "        \n",
    "        return final_out\n",
    "        \n",
    "    def last_timestep(self, outputs, lengths, bidirectional=False):\n",
    "        \"\"\"\n",
    "            Returns the last output of the LSTM taking into account the zero padding\n",
    "        \"\"\"\n",
    "        if self.bidirectional:\n",
    "            forward, backward = self.split_directions(outputs)\n",
    "            last_forward = self.last_by_index(forward, lengths)\n",
    "            last_backward = backward[:, 0, :]\n",
    "            # Concatenate and return - maybe add more functionalities like average\n",
    "            return torch.cat((last_forward, last_backward), dim=-1)\n",
    "\n",
    "        else:\n",
    "            return self.last_by_index(outputs, lengths)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_directions(outputs):\n",
    "        direction_size = int(outputs.size(-1) / 2)\n",
    "        forward = outputs[:, :, :direction_size]\n",
    "        backward = outputs[:, :, direction_size:]\n",
    "        return forward, backward\n",
    "\n",
    "    @staticmethod\n",
    "    def last_by_index(outputs, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(outputs.size(0),\n",
    "                                               outputs.size(2)).unsqueeze(1)\n",
    "        return outputs.gather(1, idx).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "366d9fc0b259ede8b45e4eaa2584054750a2e151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense1): Linear(in_features=6720, out_features=500, bias=True)\n",
       "  (dense2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs=35\n",
    "kernel_sz=3\n",
    "input_channels=1\n",
    "out_channels=1\n",
    "stride=2\n",
    "padding=2\n",
    "num_classes=10\n",
    "\n",
    "\n",
    "device=torch.device(\"cuda\")\n",
    "\n",
    "model3 = ConvNet(input_channels,out_channels,kernel_sz,stride,padding ,num_classes)\n",
    "model3.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "44ef3bb8b13fecc25bb98753ecd8b17a31e96646",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t \t Training Loss 1.9550755072946417\n",
      "Epoch: 1 \t \t Training Loss 1.7853517548678672\n",
      "Epoch: 2 \t \t Training Loss 1.6320572242344895\n",
      "Epoch: 3 \t \t Training Loss 1.5808913626082957\n",
      "Epoch: 4 \t \t Training Loss 1.494195333898884\n",
      "Epoch: 5 \t \t Training Loss 1.266007518931611\n",
      "Epoch: 6 \t \t Training Loss 1.1877651965781435\n",
      "Epoch: 7 \t \t Training Loss 1.0898732110245588\n",
      "Epoch: 8 \t \t Training Loss 1.721505875456823\n",
      "Epoch: 9 \t \t Training Loss 1.5065337852255938\n",
      "Epoch: 10 \t \t Training Loss 1.5415091873848275\n",
      "Epoch: 11 \t \t Training Loss 1.2349792122840881\n",
      "Epoch: 12 \t \t Training Loss 1.022954719523861\n",
      "Epoch: 13 \t \t Training Loss 0.9279517874325791\n",
      "Epoch: 14 \t \t Training Loss 0.8711650289901315\n",
      "Epoch: 15 \t \t Training Loss 0.7430572387290327\n",
      "Epoch: 16 \t \t Training Loss 1.0745038488139844\n",
      "Epoch: 17 \t \t Training Loss 0.6759969343061316\n",
      "Epoch: 18 \t \t Training Loss 0.3477524216860941\n",
      "Epoch: 19 \t \t Training Loss 0.2592628004208003\n",
      "Epoch: 20 \t \t Training Loss 0.7926048901799607\n",
      "Epoch: 21 \t \t Training Loss 0.5742476411061744\n",
      "Epoch: 22 \t \t Training Loss 1.1021391504431424\n",
      "Epoch: 23 \t \t Training Loss 0.33297405606263303\n",
      "Epoch: 24 \t \t Training Loss 0.1695871295994275\n",
      "Epoch: 25 \t \t Training Loss 0.16368787194768045\n",
      "Epoch: 26 \t \t Training Loss 2.034695074982839\n",
      "Epoch: 27 \t \t Training Loss 1.6886117278713069\n",
      "Epoch: 28 \t \t Training Loss 1.203775607559779\n",
      "Epoch: 29 \t \t Training Loss 0.8029146459821153\n",
      "Epoch: 30 \t \t Training Loss 0.48184639337944657\n",
      "Epoch: 31 \t \t Training Loss 0.2950177796899456\n",
      "Epoch: 32 \t \t Training Loss 0.2037796897635068\n",
      "Epoch: 33 \t \t Training Loss 0.12325685736659454\n",
      "Epoch: 34 \t \t Training Loss 0.07440264931280319\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model3.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #no need to set requires_grad=True for parameters(weights) as it done by default. Also for input requires_grad is not\n",
    "    #always necessary. So we comment the following line.\n",
    "    #with torch.autograd(): \n",
    "    model3.train()\n",
    "    #scheduler.step()\n",
    "    running_average_loss = 0\n",
    "\n",
    "    #train model in each epoch\n",
    "    for index,instance in enumerate(train_loader):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        #features,labels,lengths=instance\n",
    "        \n",
    "        features = instance[:][0].to(device)\n",
    "        labels = instance[:][1].to(device)\n",
    "        lengths = instance[:][2].to(device)\n",
    "        features = features.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        prediction_vec = model3(features,lengths)\n",
    "        prediction_vec.to(device)\n",
    "        #print(prediction_vec.shape)\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(prediction_vec,labels)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_average_loss += loss.detach().item()\n",
    "    print(\"Epoch: {} \\t \\t Training Loss {}\".format(epoch, float(running_average_loss) / (index + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2fe8a7abd1983020337f8729a88ce97e62841995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel3.eval()\\nacc = 0\\nn_samples = 0\\nwith torch.no_grad():\\n    for index, batch in enumerate(val_loader):\\n        features = batch[:][0].to(device)\\n        labels = batch[:][1].to(device)\\n        lengths = batch[:][2].to(device)\\n        features = features.type(torch.FloatTensor).to(device)\\n        #print(features.shape)\\n        out = model3(features,lengths)\\n        out = out.to(device)\\n        out_scores = F.log_softmax(out,dim=1)\\n        \\n        value, y_pred = out_scores.max(1)\\n\\n        acc += (labels == y_pred).sum().detach().item()\\n        n_samples += features.shape[0]\\n\\nprint(\"Score for validation set: \" ,acc / n_samples)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model3.eval()\n",
    "acc = 0\n",
    "n_samples = 0\n",
    "with torch.no_grad():\n",
    "    for index, batch in enumerate(val_loader):\n",
    "        features = batch[:][0].to(device)\n",
    "        labels = batch[:][1].to(device)\n",
    "        lengths = batch[:][2].to(device)\n",
    "        features = features.type(torch.FloatTensor).to(device)\n",
    "        #print(features.shape)\n",
    "        out = model3(features,lengths)\n",
    "        out = out.to(device)\n",
    "        out_scores = F.log_softmax(out,dim=1)\n",
    "        \n",
    "        value, y_pred = out_scores.max(1)\n",
    "\n",
    "        acc += (labels == y_pred).sum().detach().item()\n",
    "        n_samples += features.shape[0]\n",
    "\n",
    "print(\"Score for validation set: \" ,acc / n_samples)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "9a18566ba8875d41962e594f080136bd6527ac1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for test set:  0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "model3.eval()\n",
    "acc = 0\n",
    "n_samples = 0\n",
    "with torch.no_grad():\n",
    "    for index, batch in enumerate(test_loader):\n",
    "        features = batch[:][0].to(device)\n",
    "        labels = batch[:][1].to(device)\n",
    "        lengths = batch[:][2].to(device)\n",
    "        features = features.type(torch.FloatTensor).to(device)\n",
    "        #print(features.shape)\n",
    "        out = model3(features,lengths)\n",
    "        out = out.to(device)\n",
    "        #print(out.shape)\n",
    "        out_scores = F.log_softmax(out,dim=0)\n",
    "        \n",
    "        value, y_pred = out_scores.max(0)\n",
    "\n",
    "        acc += (labels == y_pred).sum().detach().item()\n",
    "        n_samples += features.shape[0]\n",
    "\n",
    "print(\"Score for test set: \" ,acc / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "f257d2b540d883b2aff545a0ee96fe2b5da18b94"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
