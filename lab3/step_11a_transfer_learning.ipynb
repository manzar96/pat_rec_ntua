{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "548d255c9fea24c7a4ffc87d72a02b373bf889d0"
   },
   "source": [
    "**Βήμα 11α: Transfer Learning**\n",
    " Για την υλοποίηση transfer learning η βασική ιδέα είναι ότι εκπαιδεύουμε ένα μοντέλο σε dataset το οποίο έχει μεγαλύτερο μέγεθος ώστε να εκπαιδευτεί το μοντέλο καλύτερα στο γενικότερο εύρος της πληροφορίας (τα dataset πρεπει να είναι παρόμοιο περιεχομένου). Στην συνέχεια, μετά την γενική εκπαίδευση του μοντέλου (στην οποία κρατάμε το καλύτερο με χρήση checkpoints) αφαιρούμε τα τελευταία layers τα οποία εμπεριέχουν την ειδική πληροφορία και επανεκπαιδεύουμε το μοντέλο στο δικό μας dataset (για λιγότερες εποχές) κρατώντας ίδια τα βάρη των layers που αφήσαμε και προσθέτοντας στην θέση των τελευταίων που αφαιεσαμε άλλα τα οποία αρχικοποιούνται τυχαία.\n",
    " Έτσι μαθαίνουμε τα τελευταία layers στην ειδική πλροφορία του dataset μας.\n",
    " \n",
    " Παρατηρούμε ότι το transfer learning που εφαρμόσαμε δεν είχε τόσο μεγάλη επιτυχία,καθώς δεν ήταν τόσο καλά τα αποτελέσματα οσο το βήμα 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "import os\n",
    "\n",
    "class_mapping = {\n",
    "    'Rock': 'Rock',\n",
    "    'Psych-Rock': 'Rock',\n",
    "    'Indie-Rock': None,\n",
    "    'Post-Rock': 'Rock',\n",
    "    'Psych-Folk': 'Folk',\n",
    "    'Folk': 'Folk',\n",
    "    'Metal': 'Metal',\n",
    "    'Punk': 'Metal',\n",
    "    'Post-Punk': None,\n",
    "    'Trip-Hop': 'Trip-Hop',\n",
    "    'Pop': 'Pop',\n",
    "    'Electronic': 'Electronic',\n",
    "    'Hip-Hop': 'Hip-Hop',\n",
    "    'Classical': 'Classical',\n",
    "    'Blues': 'Blues',\n",
    "    'Chiptune': 'Electronic',\n",
    "    'Jazz': 'Jazz',\n",
    "    'Soundtrack': None,\n",
    "    'International': None,\n",
    "    'Old-Time': None\n",
    "}\n",
    "\n",
    "\n",
    "def torch_train_val_split(\n",
    "        dataset, batch_train, batch_eval,\n",
    "        val_size=.2, shuffle=True, seed=42):\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    val_split = int(np.floor(val_size * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices = indices[val_split:]\n",
    "    val_indices = indices[:val_split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset,\n",
    "                              batch_size=batch_train,\n",
    "                              sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_eval,\n",
    "                            sampler=val_sampler)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def read_spectrogram(spectrogram_file, chroma=True):\n",
    "    with gzip.GzipFile(spectrogram_file, 'r') as f:\n",
    "        spectrograms = np.load(f)\n",
    "    # spectrograms contains a fused mel spectrogram and chromagram\n",
    "    # Decompose as follows\n",
    "    return spectrograms.T\n",
    "\n",
    "\n",
    "class LabelTransformer(LabelEncoder):\n",
    "    def inverse(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).inverse_transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).inverse_transform([y])\n",
    "\n",
    "    def transform(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).transform([y])\n",
    "\n",
    "        \n",
    "class PaddingTransform(object):\n",
    "    def __init__(self, max_length, padding_value=0):\n",
    "        self.max_length = max_length\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def __call__(self, s):\n",
    "        if len(s) == self.max_length:\n",
    "            return s\n",
    "\n",
    "        if len(s) > self.max_length:\n",
    "            return s[:self.max_length]\n",
    "\n",
    "        if len(s) < self.max_length:\n",
    "            s1 = copy.deepcopy(s)\n",
    "            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n",
    "            s1 = np.vstack((s1, pad))\n",
    "            return s1\n",
    "\n",
    "        \n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, path, class_mapping=None, train=True, max_length=-1):\n",
    "        t = 'train' if train else 'test'\n",
    "        p = os.path.join(path, t)\n",
    "        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n",
    "        #print(self.index)\n",
    "        self.files, labels = self.get_files_labels(self.index, class_mapping)\n",
    "        self.feats = [read_spectrogram(os.path.join(p, f)) for f in self.files]\n",
    "        self.feat_dim = self.feats[0].shape[1]\n",
    "        self.lengths = [len(i) for i in self.feats]\n",
    "        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n",
    "        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n",
    "        self.label_transformer = LabelTransformer()\n",
    "        if isinstance(labels, (list, tuple)):\n",
    "            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n",
    "\n",
    "    def get_files_labels(self, txt, class_mapping):\n",
    "        with open(txt, 'r') as fd:\n",
    "            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n",
    "        files, labels = [], []\n",
    "        for l in lines:\n",
    "            label = l[1]\n",
    "            if class_mapping:\n",
    "                label = class_mapping[l[1]]\n",
    "            if not label:\n",
    "                continue\n",
    "            files.append(l[0])\n",
    "            labels.append(label)\n",
    "        return files, labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        l = min(self.lengths[item], self.max_length)\n",
    "        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "deaf2db6e84341b024f7e7bd1d007a969b864513"
   },
   "outputs": [],
   "source": [
    "BATCH_SZ=32\n",
    "\n",
    "specs = SpectrogramDataset('../input/data/data/fma_genre_spectrograms/', train=True, class_mapping=class_mapping, max_length=-1)\n",
    "train_loader, val_loader = torch_train_val_split(specs, BATCH_SZ ,BATCH_SZ, val_size=.33)\n",
    "test_loader = DataLoader(SpectrogramDataset('../input/data/data/fma_genre_spectrograms/', train=False, class_mapping=class_mapping, max_length=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f6149fd0414044a3ff716c89fcaa072695169ef0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,input_channels, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 4, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16 , 32 , kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        \n",
    "        self.dense1= nn.Linear(6720,500) \n",
    "        self.dense2 = nn.Linear(500,10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        #print(x.shape)\n",
    "        x.unsqueeze_(1)\n",
    "        #print(x.shape)\n",
    "        out1 = self.layer1(x)\n",
    "        #print(out1.shape)\n",
    "        out2= self.layer2(out1)\n",
    "        #print(out2.shape)\n",
    "        out3= self.layer3(out2)\n",
    "        #print(out3.shape)\n",
    "        out4= self.layer4(out3)\n",
    "        #print(out4.shape)\n",
    "        \n",
    "    \n",
    "        out_flat=out4.reshape(-1,out4.size(1)*out4.size(2)*out4.size(3))\n",
    "        #print(out_flat.shape)\n",
    "        \n",
    "        \n",
    "        #implementing fully connected layers\n",
    "        \n",
    "        hidden_out = self.dense1(out_flat)\n",
    "        final_out = self.dense2(hidden_out)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "c31c852882f6228aab020ac8ecc9181b673c1554"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "1aae0cedbcc9a382efa07ddc9329c93a749056bb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Trainer_with_Checkpoints():\n",
    "    def __init__(self,validate_every,metrics,max_epochs,patience=10):\n",
    "    \n",
    "        self.validate_every=validate_every\n",
    "        self.metrics = metrics\n",
    "        self.patience=patience\n",
    "        self.best_score=None\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        \n",
    "    def validate_accuracy(self,mymodel,validation_batches):\n",
    "        with torch.no_grad():\n",
    "            mymodel.eval()\n",
    "            num_correct=0\n",
    "            num_samples=0\n",
    "            with torch.no_grad():\n",
    "                for index, instance in enumerate(validation_batches):\n",
    "                    features = instance[:][0].to(device)\n",
    "                    labels = instance[:][1].to(device)\n",
    "                    lengths = instance[:][2].to(device)\n",
    "                    features = features.type(torch.FloatTensor).to(device)\n",
    "                    \n",
    "                    out = mymodel(features)\n",
    "                    out_scores = F.log_softmax(out,dim=1)\n",
    "                    value, y_pred = out_scores.max(1)\n",
    "\n",
    "                    num_correct += (labels == y_pred).sum().detach().item()\n",
    "                    num_samples += features.shape[0]\n",
    "\n",
    "                print(\"Score for validation set: \" ,num_correct / num_samples)\n",
    "        return num_correct/num_samples\n",
    "\n",
    "    \n",
    "    def checkpoint(self,mymodel,myoptimizer,epoch,checkpointdir,myscheduler=None):\n",
    "        \n",
    "        #if myscheduler is not None:\n",
    "         #   state = {'epoch': epoch + 1,'state_dict': mymodel.state_dict(),\n",
    "       #              'optim_dict' : myoptimizer.state_dict(),'scheduler_dict' : myscheduler.state_dict()}\n",
    "        #else:\n",
    "        #    state = {'epoch': epoch + 1,'state_dict': mymodel.state_dict(),'optim_dict' : myoptimizer.state_dict()}\n",
    "        \n",
    "        #utils.save_checkpoint(state,checkpoint=self.checkpointdir) # path to folder\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': mymodel.state_dict(),\n",
    "            'optimizer_state_dict': myoptimizer.state_dict(),\n",
    "            }, checkpointdir)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def train_model(self,mymodel,myoptimizer,myloss_function,training_batches,validation_batches,\n",
    "                    checkpointdir,myscheduler=None):\n",
    "        \n",
    "        self.best_score=None\n",
    "        counter =0\n",
    "        device=torch.device(\"cuda\")\n",
    "        if self.patience < 1:\n",
    "            raise ValueError(\"Argument patience should be positive integer\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            #no need to set requires_grad=True for parameters(weights) as it done by default. Also for input requires_grad is not\n",
    "            #always necessary. So we comment the following line.\n",
    "            #with torch.autograd(): \n",
    "            mymodel.train()\n",
    "\n",
    "            if myscheduler is not None:\n",
    "                myscheduler.step()\n",
    "\n",
    "            running_average_loss = 0\n",
    "\n",
    "\n",
    "            #train model in each epoch\n",
    "            for index,instance in enumerate(training_batches):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                features = instance[:][0].to(device)\n",
    "                labels = instance[:][1].to(device)\n",
    "                lengths = instance[:][2].to(device)\n",
    "                features = features.type(torch.FloatTensor).to(device)\n",
    "                \n",
    "                myoptimizer.zero_grad()\n",
    "                \n",
    "                prediction_vec = mymodel(features)\n",
    "                prediction_vec.to(device)\n",
    "                \n",
    "                myloss = myloss_function(prediction_vec,labels)\n",
    "                myloss.backward(retain_graph=True)\n",
    "                myoptimizer.step()\n",
    "                running_average_loss += myloss.detach().item()\n",
    "                if index % 100 == 0:\n",
    "                    print(\"Epoch: {} \\t Batch: {} \\t Training Loss {}\".format(epoch, index, float(running_average_loss) / (index + 1)))\n",
    "               \n",
    "            if epoch==self.max_epochs-1:\n",
    "                print(\"yyyyyeaaaaahhhh\")\n",
    "                if 'accuracy' in self.metrics:\n",
    "                    score = self.validate_accuracy(mymodel,validation_batches)\n",
    "\n",
    "                if self.best_score is None:\n",
    "                    self.best_score = score\n",
    "                    self.checkpoint(mymodel,myoptimizer,epoch,checkpointdir,myscheduler)\n",
    "                    print(\"checkpoint done!\")\n",
    "                    \n",
    "                elif score < self.best_score:\n",
    "                    counter += 1\n",
    "                    if counter >= self.patience:\n",
    "                        print(\"EarlyStopping: Stop training\")\n",
    "                        return\n",
    "                else:\n",
    "                    #found better state in our model\n",
    "                    self.best_score = score\n",
    "                    counter = 0\n",
    "                    #checkpoint\n",
    "                    self.checkpoint(mymodel,myoptimizer,epoch,checkpointdir,myscheduler)\n",
    "                    print(\"checkpoint done!\")\n",
    "            \n",
    "            if epoch % self.validate_every == 0:\n",
    "                if 'accuracy' in self.metrics:\n",
    "                    score = self.validate_accuracy(mymodel,validation_batches)\n",
    "\n",
    "                if self.best_score is None:\n",
    "                    self.best_score = score\n",
    "                    #checkpoint\n",
    "                    self.checkpoint(mymodel,myoptimizer,epoch,checkpointdir,myscheduler)\n",
    "                    print(\"checkpoint done!\")\n",
    "                    \n",
    "                elif score < self.best_score:\n",
    "                    counter += 1\n",
    "                    if counter >= self.patience:\n",
    "                        print(\"EarlyStopping: Stop training\")\n",
    "                        return\n",
    "                    \n",
    "                else:\n",
    "                    #found better state in our model\n",
    "                    self.best_score = score\n",
    "                    counter = 0\n",
    "                    #checkpoint\n",
    "                    self.checkpoint(mymodel,myoptimizer,epoch,checkpointdir,myscheduler)\n",
    "                    print(\"checkpoint done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "789f0becae8f285991ec301158804956101cccb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 0 \t Training Loss 2.3606491088867188\n",
      "Score for validation set:  0.23684210526315788\n",
      "checkpoint done!\n",
      "Epoch: 1 \t Batch: 0 \t Training Loss 2.246367931365967\n",
      "Epoch: 2 \t Batch: 0 \t Training Loss 2.0726478099823\n",
      "Epoch: 3 \t Batch: 0 \t Training Loss 1.9512250423431396\n",
      "Epoch: 4 \t Batch: 0 \t Training Loss 1.7854173183441162\n",
      "Epoch: 5 \t Batch: 0 \t Training Loss 2.1310973167419434\n",
      "Score for validation set:  0.12763157894736843\n",
      "Epoch: 6 \t Batch: 0 \t Training Loss 1.5493055582046509\n",
      "Epoch: 7 \t Batch: 0 \t Training Loss 2.8074753284454346\n",
      "Epoch: 8 \t Batch: 0 \t Training Loss 1.6358164548873901\n",
      "Epoch: 9 \t Batch: 0 \t Training Loss 1.408928394317627\n",
      "Epoch: 10 \t Batch: 0 \t Training Loss 0.538772702217102\n",
      "Score for validation set:  0.13026315789473683\n",
      "Epoch: 11 \t Batch: 0 \t Training Loss 1.1837912797927856\n",
      "Epoch: 12 \t Batch: 0 \t Training Loss 0.4012756943702698\n",
      "Epoch: 13 \t Batch: 0 \t Training Loss 0.8012294173240662\n",
      "Epoch: 14 \t Batch: 0 \t Training Loss 1.2533838748931885\n",
      "Epoch: 15 \t Batch: 0 \t Training Loss 0.09326266497373581\n",
      "Score for validation set:  0.3592105263157895\n",
      "checkpoint done!\n",
      "Epoch: 16 \t Batch: 0 \t Training Loss 0.07505553960800171\n",
      "Epoch: 17 \t Batch: 0 \t Training Loss 0.01906539499759674\n",
      "Epoch: 18 \t Batch: 0 \t Training Loss 0.026048406958580017\n",
      "Epoch: 19 \t Batch: 0 \t Training Loss 0.01648862659931183\n",
      "Epoch: 20 \t Batch: 0 \t Training Loss 0.01260879635810852\n",
      "Score for validation set:  0.4052631578947368\n",
      "checkpoint done!\n",
      "Epoch: 21 \t Batch: 0 \t Training Loss 0.016264572739601135\n",
      "Epoch: 22 \t Batch: 0 \t Training Loss 0.011804267764091492\n",
      "Epoch: 23 \t Batch: 0 \t Training Loss 0.011162146925926208\n",
      "Epoch: 24 \t Batch: 0 \t Training Loss 0.00624537467956543\n",
      "Epoch: 25 \t Batch: 0 \t Training Loss 0.008829787373542786\n",
      "Score for validation set:  0.4368421052631579\n",
      "checkpoint done!\n",
      "Epoch: 26 \t Batch: 0 \t Training Loss 0.008028581738471985\n",
      "Epoch: 27 \t Batch: 0 \t Training Loss 0.008331194519996643\n",
      "Epoch: 28 \t Batch: 0 \t Training Loss 0.009419545531272888\n",
      "Epoch: 29 \t Batch: 0 \t Training Loss 0.004989355802536011\n",
      "Epoch: 30 \t Batch: 0 \t Training Loss 0.0053161680698394775\n",
      "Score for validation set:  0.43026315789473685\n",
      "Epoch: 31 \t Batch: 0 \t Training Loss 0.0027925819158554077\n",
      "Epoch: 32 \t Batch: 0 \t Training Loss 0.004939943552017212\n",
      "Epoch: 33 \t Batch: 0 \t Training Loss 0.0053441524505615234\n",
      "Epoch: 34 \t Batch: 0 \t Training Loss 0.004757225513458252\n",
      "Epoch: 35 \t Batch: 0 \t Training Loss 0.008615776896476746\n",
      "Score for validation set:  0.4381578947368421\n",
      "checkpoint done!\n",
      "Epoch: 36 \t Batch: 0 \t Training Loss 0.005249395966529846\n",
      "Epoch: 37 \t Batch: 0 \t Training Loss 0.002304390072822571\n",
      "Epoch: 38 \t Batch: 0 \t Training Loss 0.003843963146209717\n",
      "Epoch: 39 \t Batch: 0 \t Training Loss 0.002150222659111023\n",
      "yyyyyeaaaaahhhh\n",
      "Score for validation set:  0.43026315789473685\n"
     ]
    }
   ],
   "source": [
    "VALIDATE_EVERY=5\n",
    "METRICS='accuracy'\n",
    "MAX_EPOCHS=40\n",
    "PATIENCE=3\n",
    "\n",
    "\n",
    "\n",
    "CHECKDIR='./model_tranfer.pt'\n",
    "\n",
    "input_channels=1\n",
    "num_classes=10\n",
    "device=torch.device(\"cuda\")\n",
    "model3 = ConvNet(input_channels,num_classes)\n",
    "model3.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model3.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "trainer = Trainer_with_Checkpoints(validate_every=VALIDATE_EVERY,metrics=METRICS,max_epochs=MAX_EPOCHS,patience=PATIENCE)\n",
    "\n",
    "trainer.train_model(mymodel=model3,myoptimizer=optimizer,myloss_function=criterion,training_batches=train_loader,\n",
    "                    validation_batches=val_loader,checkpointdir=CHECKDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c65b73a4ebd6f8f40385e35e0377e39a10510a7d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class_mapping = {\n",
    "    'Rock': 'Rock',\n",
    "    'Psych-Rock': 'Rock',\n",
    "    'Indie-Rock': None,\n",
    "    'Post-Rock': 'Rock',\n",
    "    'Psych-Folk': 'Folk',\n",
    "    'Folk': 'Folk',\n",
    "    'Metal': 'Metal',\n",
    "    'Punk': 'Metal',\n",
    "    'Post-Punk': None,\n",
    "    'Trip-Hop': 'Trip-Hop',\n",
    "    'Pop': 'Pop',\n",
    "    'Electronic': 'Electronic',\n",
    "    'Hip-Hop': 'Hip-Hop',\n",
    "    'Classical': 'Classical',\n",
    "    'Blues': 'Blues',\n",
    "    'Chiptune': 'Electronic',\n",
    "    'Jazz': 'Jazz',\n",
    "    'Soundtrack': None,\n",
    "    'International': None,\n",
    "    'Old-Time': None\n",
    "}\n",
    "\n",
    "\n",
    "def torch_train_val_split(\n",
    "        dataset, batch_train, batch_eval,\n",
    "        val_size=.2, shuffle=True, seed=42):\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    val_split = int(np.floor(val_size * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices = indices[val_split:]\n",
    "    val_indices = indices[:val_split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset,\n",
    "                              batch_size=batch_train,\n",
    "                              sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_eval,\n",
    "                            sampler=val_sampler)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def read_spectrogram(spectrogram_file, chroma=True):\n",
    "    with gzip.GzipFile(spectrogram_file, 'r') as f:\n",
    "        spectrograms = np.load(f)\n",
    "    # spectrograms contains a fused mel spectrogram and chromagram\n",
    "    # Decompose as follows\n",
    "    return spectrograms.T\n",
    "\n",
    "\n",
    "class LabelTransformer(LabelEncoder):\n",
    "    def inverse(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).inverse_transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).inverse_transform([y])\n",
    "\n",
    "    def transform(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).transform([y])\n",
    "\n",
    "        \n",
    "class PaddingTransform(object):\n",
    "    def __init__(self, max_length, padding_value=0):\n",
    "        self.max_length = max_length\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def __call__(self, s):\n",
    "        if len(s) == self.max_length:\n",
    "            return s\n",
    "\n",
    "        if len(s) > self.max_length:\n",
    "            return s[:self.max_length]\n",
    "\n",
    "        if len(s) < self.max_length:\n",
    "            s1 = copy.deepcopy(s)\n",
    "            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n",
    "            s1 = np.vstack((s1, pad))\n",
    "            return s1\n",
    "\n",
    "        \n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, path, class_mapping=None, train=True, max_length=-1):\n",
    "        t = 'train' if train else 'test'\n",
    "        p = os.path.join(path, t)\n",
    "        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n",
    "        self.files, labels = self.get_files_labels(self.index, class_mapping)\n",
    "        #print(self.files)\n",
    "        \n",
    "        self.feats = [read_spectrogram(os.path.join(p, f+\".fused.full.npy.gz\")) for f in self.files]\n",
    "        self.feat_dim = self.feats[0].shape[1]\n",
    "        self.lengths = [len(i) for i in self.feats]\n",
    "        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n",
    "        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n",
    "        #self.label_transformer = LabelTransformer()\n",
    "        #if isinstance(labels, (list, tuple)):\n",
    "            #self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n",
    "        self.labels=labels\n",
    "    def get_files_labels(self, txt, class_mapping):\n",
    "        with open(txt, 'r') as fd:\n",
    "            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n",
    "            \n",
    "        files, labels = [], []\n",
    "        for l in lines:\n",
    "            l=l[0].split(\",\")\n",
    "            b=l[1:]\n",
    "            b = list(map(float,b))\n",
    "            files.append(l[0])\n",
    "            \n",
    "            labels.append(b)\n",
    "        return files, labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        l = min(self.lengths[item], self.max_length)\n",
    "        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8a40b4bd3a4086e405ea0644d3daa95120eefca3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_SZ=32\n",
    "\n",
    "specs = SpectrogramDataset('../input/data/data/multitask_dataset/', train=True, class_mapping=class_mapping, max_length=-1)\n",
    "train_loader, val_loader = torch_train_val_split(specs, BATCH_SZ ,BATCH_SZ, val_size=.33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6fac8d636046b2ec505788ca5875ce94a96a1a43"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(CHECKDIR)\n",
    "\n",
    "model3.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ab691d0e37c66c7e00674a0eae3a84ae7b3d4068"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense1): Linear(in_features=6720, out_features=500, bias=True)\n",
       "  (dense2): Linear(in_features=500, out_features=50, bias=True)\n",
       "  (dense3): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model3.parameters():\n",
    "    param.requires_grad=False\n",
    "    \n",
    "model3.dense1= nn.Linear(6720,500) \n",
    "model3.dense2 = nn.Linear(500,50)\n",
    "model3.dense3 = nn.Linear(50,1)\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "008d92946b2688d40d650008e2bb649d433cc41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t \t Training Loss 2.687690374751886\n",
      "Epoch: 1 \t \t Training Loss 1.189791811009248\n",
      "Epoch: 2 \t \t Training Loss 1.2360031033555667\n",
      "Epoch: 3 \t \t Training Loss 1.3810958390434582\n",
      "Epoch: 4 \t \t Training Loss 1.0928535958131154\n",
      "Epoch: 5 \t \t Training Loss 0.8533485345542431\n",
      "Epoch: 6 \t \t Training Loss 0.6935931220650673\n",
      "Epoch: 7 \t \t Training Loss 0.4840547790129979\n",
      "Epoch: 8 \t \t Training Loss 0.3497561203936736\n",
      "Epoch: 9 \t \t Training Loss 0.2464602943509817\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "\n",
    "num_epochs=10\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters())\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #no need to set requires_grad=True for parameters(weights) as it done by default. Also for input requires_grad is not\n",
    "    #always necessary. So we comment the following line.\n",
    "    #with torch.autograd(): \n",
    "    model3.train()\n",
    "    #scheduler.step()\n",
    "    running_average_loss = 0\n",
    "\n",
    "    #train model in each epoch\n",
    "    for index,instance in enumerate(train_loader):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        #features,labels,lengths=instance\n",
    "        \n",
    "        features = instance[:][0].to(device)\n",
    "        labels = instance[:][1]\n",
    "        valence_labels = labels[0].type(torch.FloatTensor).to(device)\n",
    "        energy_labels = labels[1].type(torch.FloatTensor).to(device)\n",
    "        dance_labels = labels[2].type(torch.FloatTensor).to(device)\n",
    "        lengths = instance[:][2].to(device)\n",
    "        features = features.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        prediction_vec = model3(features)\n",
    "        prediction_vec.to(device)\n",
    "        \n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        energy_labels = energy_labels.unsqueeze(1)\n",
    "        \n",
    "        loss = criterion(prediction_vec,energy_labels)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_average_loss += loss.detach().item()\n",
    "    print(\"Epoch: {} \\t \\t Training Loss {}\".format(epoch, float(running_average_loss) / (index + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "ed0960b7f22eac1784d28277669cf5100d2c1070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearnman's correlation for CNN-2d in validation set (predicting energy):  0.2647716075704687\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "model3.eval()\n",
    "\n",
    "n_samples = 0\n",
    "SE = 0\n",
    "spearman=[]\n",
    "running_average_loss=0\n",
    "with torch.no_grad():\n",
    "    for index, instance in enumerate(val_loader):\n",
    "        features = instance[:][0].to(device)\n",
    "        labels = instance[:][1]\n",
    "        valence_labels = labels[0].type(torch.FloatTensor).to(device)\n",
    "        energy_labels = labels[1].type(torch.FloatTensor).to(device)\n",
    "        dance_labels = labels[2].type(torch.FloatTensor).to(device)\n",
    "        lengths = instance[:][2].to(device)\n",
    "        features = features.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        \n",
    "    \n",
    "        out = model3(features)\n",
    "        out = out.to(device)\n",
    "        #print(out)\n",
    "        #print(valence_labels)\n",
    "\n",
    "        energy_labels = energy_labels.unsqueeze(1)\n",
    "\n",
    "        \n",
    "        spearman.append(stats.spearmanr(energy_labels.cpu().squeeze(),out.cpu().squeeze(),axis=0)[0])\n",
    "\n",
    "print(\"Spearnman's correlation for CNN-2d in validation set (predicting energy): \" , np.mean(spearman) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "c311fe96ce399b09f91e7d1bea4409a57a489d84"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
